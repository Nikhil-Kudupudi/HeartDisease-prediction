{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing dataset\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from lightgbm import LGBMClassifier\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import confusion_matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset=pd.read_csv('cleve.csv')\n",
    "dataset.head()\n",
    "X=dataset.iloc[:,:-1].values\n",
    "y=dataset.iloc[:,13].values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#imputation for data cleaning i.e; replace missing values with mean values\n",
    "from sklearn.impute import SimpleImputer\n",
    "Imputation=SimpleImputer(strategy=\"mean\",missing_values=np.nan, fill_value=None, verbose=0, copy=True, add_indicator=False)\n",
    "X[:,11:13]=Imputation.fit_transform(X[:,11:13])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#divide dataset into train and test set\n",
    "from sklearn.model_selection import train_test_split,GridSearchCV\n",
    "X_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.2,random_state=9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Scaling is a technique of feature Scaling process\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler=StandardScaler(with_mean=True,with_std=True)\n",
    "X_train=scaler.fit_transform(X_train)\n",
    "X_test=scaler.fit_transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8524590163934426 0.8516483516483517\n"
     ]
    }
   ],
   "source": [
    "#logistic Regression\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score,roc_auc_score\n",
    "lr=LogisticRegression(penalty=\"l2\",solver=\"lbfgs\",random_state=42)\n",
    "lr.fit(X_train,y_train)\n",
    "ypred=lr.predict(X_test)\n",
    "print(accuracy_score(y_test,ypred),roc_auc_score(y_test,ypred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8524590163934426 0.8516483516483517\n"
     ]
    }
   ],
   "source": [
    "##naive Bayes \n",
    "from sklearn.naive_bayes import BernoulliNB,GaussianNB\n",
    "#Cb=GaussianNB()\n",
    "Cb=BernoulliNB()\n",
    "Cb.fit(X_train,y_train)\n",
    "y_pred=Cb.predict(X_test)\n",
    "print(accuracy_score(y_test,y_pred),roc_auc_score(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7704918032786885 0.7604395604395604\n"
     ]
    }
   ],
   "source": [
    "#DecisionTree\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "Dt=DecisionTreeClassifier(criterion='gini',max_depth=20,splitter=\"best\",min_samples_split=3,random_state=42)\n",
    "Dt.fit(X_train,y_train)\n",
    "y_pred=Dt.predict(X_test)\n",
    "print(accuracy_score(y_test,y_pred),roc_auc_score(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8360655737704918 0.8373626373626373\n"
     ]
    }
   ],
   "source": [
    "#K Nearest negibhor\n",
    "from sklearn.impute import KNNImputer\n",
    "Kimputer=KNNImputer()\n",
    "Kimputer.fit_transform(X_train,y_train)\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "KNN=KNeighborsClassifier(metric='minkowski',algorithm='kd_tree')\n",
    "KNN.fit(X_train,y_train)\n",
    "y_pred=KNN.predict(X_test)\n",
    "print(accuracy_score(y_test,y_pred),roc_auc_score(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8524590163934426 0.8516483516483517\n"
     ]
    }
   ],
   "source": [
    "#Support vector Classifier\n",
    "from sklearn.svm import LinearSVC\n",
    "svc=LinearSVC(penalty='l2',C=2,verbose=0,max_iter=10000)\n",
    "svc.fit(X_train,y_train)\n",
    "y_pred1=svc.predict(X_test)\n",
    "print(accuracy_score(y_test,y_pred1),roc_auc_score(y_test,y_pred1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.2601383857786869 -0.06370871811263079\n",
      "0.8362637362637362\n",
      "0.7377049180327869\n"
     ]
    }
   ],
   "source": [
    "#Support Vector Regression\n",
    "from sklearn.svm import LinearSVR\n",
    "from sklearn.metrics import mean_squared_error,r2_score,f1_score\n",
    "svr=LinearSVR(dual=True,max_iter=10000)\n",
    "svr.fit(X_train,y_train)\n",
    "y_pred=svr.predict(X_test)\n",
    "print(mean_squared_error(y_test,y_pred),r2_score(y_test,y_pred))\n",
    "print(roc_auc_score(y_test,y_pred))\n",
    "print(accuracy_score(y_test,[round(i) for i in y_pred]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9180327868852459\n"
     ]
    }
   ],
   "source": [
    "#HRFLM\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "rf=RandomForestClassifier(n_estimators=2,min_samples_split=2,random_state=42)\n",
    "lr=LogisticRegression(penalty=\"l2\",solver=\"lbfgs\")\n",
    "enc=OneHotEncoder()\n",
    "rf.fit(X_train,y_train)\n",
    "enc.fit(rf.apply(X_train))\n",
    "lr.fit(enc.transform(rf.apply(X_test)),y_test)\n",
    "y_pred=lr.predict_proba(enc.transform(rf.apply(X_test)))[:,1]\n",
    "print(accuracy_score(y_test,[round(i) for i in y_pred]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 36 candidates, totalling 180 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GBDT ROCAUC score: 0.84\n",
      "[[29  6]\n",
      " [ 4 22]]\n",
      "0.8360655737704918\n",
      "0.16393442622950818 0.3296703296703296\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done 180 out of 180 | elapsed:    9.7s finished\n"
     ]
    }
   ],
   "source": [
    "params = {\n",
    "    'num_leaves': [10, 30, 40, 50, 60, 70],\n",
    "    'learning_rate': [0.05, 0.01],\n",
    "    'n_estimators': [100, 300, 500],\n",
    "    'subsample': [0.95],\n",
    "    'colsample_bytree': [0.95],\n",
    "    'n_jobs': [7],\n",
    "    'random_state': [42]\n",
    "}\n",
    "from sklearn.metrics import roc_auc_score,roc_curve,r2_score,mean_squared_error\n",
    "\n",
    "gcv = GridSearchCV(LGBMClassifier(), params, cv=5, verbose=1).fit(X_train, y_train)\n",
    "gbm = gcv.best_estimator_\n",
    "gbm_pred = gbm.predict(X_test)\n",
    "print(\"GBDT ROCAUC score: {:.2f}\".format(roc_auc_score(y_test, gbm_pred)))\n",
    "print(confusion_matrix(y_test,gbm_pred))\n",
    "print(accuracy_score(y_test,gbm_pred))\n",
    "print(mean_squared_error(y_test,gbm_pred),r2_score(y_test,gbm_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The AUC of GBT+LR is: 0.8516483516483517\n",
      "0.7704918032786885\n"
     ]
    }
   ],
   "source": [
    "##GBDT+LR\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "X_train, X_train_lr, y_train, y_train_lr = train_test_split(X_train, y_train, test_size=0.2)\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "grd = GradientBoostingClassifier(n_estimators=10)\n",
    "grd_enc = OneHotEncoder()\n",
    "grd_lm = LogisticRegression(solver='lbfgs', max_iter=1000)\n",
    "\n",
    "grd.fit(X_train, y_train)\n",
    "grd_enc.fit(grd.apply(X_train)[:, :, 0])\n",
    "grd_lm.fit(grd_enc.transform(grd.apply(X_train_lr)[:, :, 0]), y_train_lr)\n",
    " \n",
    "y_pred_grd_lm = grd_lm.predict_proba(\n",
    "grd_enc.transform(grd.apply(X_test)[:, :, 0]))[:, 1]\n",
    "fpr_grd_lm, tpr_grd_lm, _ = roc_curve(y_test, y_pred_grd_lm)\n",
    "print(\"The AUC of GBT+LR is:\", roc_auc_score(y_test, y_pred_grd_lm))\n",
    "\n",
    "l=[round(i) for i in y_pred_grd_lm]\n",
    "print(accuracy_score(y_test,l))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[18:48:15] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.9016393442622951\n"
     ]
    }
   ],
   "source": [
    "import xgboost as xgb\n",
    "xgb = xgb.XGBClassifier(nthread=4,     #Meaning: When nthread=-1, use all CPUs for parallel operation (default), when nthread=1, use 1 CPU for operation.\n",
    "                          learning_rate=0.001,    #Meaning: learning rate, which controls the step size when the weight is updated in each iteration, the default is 0.3. Tuning: The smaller the value, the slower the training. The typical value is 0.01-0.2.\n",
    "                          n_estimators=50,       #Meaning: The total number of iterations, that is, the number of decision trees\n",
    "                          max_depth=5,         #Meaning: the depth of the tree, the default value is 6, the typical value is 3-10. Parameter tuning: the larger the value, the easier it is to overfit; the smaller the value, the easier it is to underfit\n",
    "                          gamma=0,               #Meaning: The penalty term coefficient specifies the minimum loss function drop value required for node splitting.\n",
    "                          subsample=0.9,       #Meaning: When training each tree, the proportion of data used in the total training set. The default value is 1, and the typical value is 0.5-1. Tuning: Prevent overfitting.\n",
    "                          colsample_bytree=0.5,use_label_encoder=False) #When training each tree, the proportion of features used in total features. The default value is 1, and the typical value is 0.5-1. Tuning: Prevent overfitting.\n",
    " \n",
    "xgb_enc = OneHotEncoder()\n",
    "xgb_lm = LogisticRegression(solver='lbfgs', max_iter=10000)\n",
    "xgb.fit(X_train, y_train)\n",
    "xgb_enc.fit(xgb.apply(X_train))\n",
    "xgb_lm.fit(xgb_enc.transform(xgb.apply(X_train_lr)), y_train_lr) \n",
    "y_pred_xgb_lm = xgb_lm.predict_proba(xgb_enc.transform(xgb.apply(X_test)))[:, 1]\n",
    "\n",
    "fpr_xgb_lm, tpr_xgb_lm, _ = roc_curve(y_test, y_pred_xgb_lm)\n",
    "#print(\"The AUC of xgboost+LR is:\", roc_auc_score(y_test, y_pred_xgb_lm))\n",
    "l=[round(i) for i in y_pred_xgb_lm]\n",
    "print(accuracy_score(y_test,l))\n",
    "#print(mean_squared_error(y_test,y_pred_xgb_lm),r2_score(y_test,y_pred_xgb_lm))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
