{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing dataset\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from lightgbm import LGBMClassifier\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "from sklearn.metrics import accuracy_score,roc_auc_score\n",
    "import sklearn\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import plotly as plot\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "import plotly.offline as pyo\n",
    "from plotly.offline import init_notebook_mode,plot,iplot\n",
    "dataset=pd.read_csv('cleve.csv')\n",
    "\n",
    "X=dataset.iloc[:,:-1].values\n",
    "y=dataset.iloc[:,13].values\n",
    "dataset.head()\n",
    "#imputation for data cleaning i.e; replace missing values with mean values\n",
    "from sklearn.impute import SimpleImputer\n",
    "Imputation=SimpleImputer(strategy=\"mean\",missing_values=np.nan, fill_value=None, verbose=0, copy=True, add_indicator=False)\n",
    "X[:,11:13]=Imputation.fit_transform(X[:,11:13])\n",
    "\n",
    "#divide dataset into train and test set\n",
    "from sklearn.model_selection import train_test_split,GridSearchCV\n",
    "X_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.2,random_state=63)\n",
    "#Scaling is a technique of feature Scaling process\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler=StandardScaler(with_mean=True,with_std=True)\n",
    "X_train=scaler.fit_transform(X_train)\n",
    "X_test=scaler.fit_transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[18:14:28] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "83.60655737704919\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "X_train, X_train_lr, y_train, y_train_lr = train_test_split(X_train, y_train, test_size=0.2,random_state=43)\n",
    "import xgboost as xgb\n",
    "xgb = xgb.XGBClassifier(nthread=4,     #Meaning: When nthread=-1, use all CPUs for parallel operation (default), when nthread=1, use 1 CPU for operation.\n",
    "                          learning_rate=0.001,    #Meaning: learning rate, which controls the step size when the weight is updated in each iteration, the default is 0.3. Tuning: The smaller the value, the slower the training. The typical value is 0.01-0.2.\n",
    "                          n_estimators=50,       #Meaning: The total number of iterations, that is, the number of decision trees\n",
    "                          max_depth=5,         #Meaning: the depth of the tree, the default value is 6, the typical value is 3-10. Parameter tuning: the larger the value, the easier it is to overfit; the smaller the value, the easier it is to underfit\n",
    "                          gamma=0,               #Meaning: The penalty term coefficient specifies the minimum loss function drop value required for node splitting.\n",
    "                          subsample=0.9,       #Meaning: When training each tree, the proportion of data used in the total training set. The default value is 1, and the typical value is 0.5-1. Tuning: Prevent overfitting.\n",
    "                          colsample_bytree=0.5,use_label_encoder=False) #When training each tree, the proportion of features used in total features. The default value is 1, and the typical value is 0.5-1. Tuning: Prevent overfitting.\n",
    " \n",
    "xgb_enc = OneHotEncoder()\n",
    "xgb_lm = LogisticRegression(solver='lbfgs', max_iter=10000)\n",
    "xgb.fit(X_train, y_train)\n",
    "xgb_enc.fit(xgb.apply(X_train))\n",
    "xgb_lm.fit(xgb_enc.transform(xgb.apply(X_train_lr)), y_train_lr) \n",
    "y_pred_xgb_lm = xgb_lm.predict_proba(xgb_enc.transform(xgb.apply(X_test)))[:, 1]\n",
    "\n",
    "#fpr_xgb_lm, tpr_xgb_lm, _ = roc_curve(y_test, y_pred_xgb_lm)\n",
    "#print(\"The AUC of xgboost+LR is:\", roc_auc_score(y_test, y_pred_xgb_lm))\n",
    "l=[round(i) for i in y_pred_xgb_lm]\n",
    "print(accuracy_score(y_test,l)*100)\n",
    "#print(mean_squared_error(y_test,y_pred_xgb_lm),r2_score(y_test,y_pred_xgb_lm))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
